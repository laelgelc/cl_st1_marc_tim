{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0dbda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data (we assume that we have one sentence per line and that the tokens are separated by whitespace)\n",
    "\n",
    "with open(\"data/wue15.txt\", \"r\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c113827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome !\\n',\n",
       " 'Hello , , and welcome to Wikipedia !\\n',\n",
       " 'Thank you for your contributions .\\n',\n",
       " 'I hope you like the place and decide to stay .\\n',\n",
       " 'Here are a few good links for newcomers :\\n',\n",
       " 'The five pillars of Wikipedia\\n',\n",
       " 'How to edit a page\\n',\n",
       " 'Help pages\\n',\n",
       " 'Tutorial\\n',\n",
       " 'How to write a great article\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see, the data is a list of strings, where each string is a line from the file.\n",
    "# Let's print the first 10 lines to see what we have.\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83884de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Welcome', '!'],\n",
       " ['Hello', ',', ',', 'and', 'welcome', 'to', 'Wikipedia', '!'],\n",
       " ['Thank', 'you', 'for', 'your', 'contributions', '.'],\n",
       " ['I',\n",
       "  'hope',\n",
       "  'you',\n",
       "  'like',\n",
       "  'the',\n",
       "  'place',\n",
       "  'and',\n",
       "  'decide',\n",
       "  'to',\n",
       "  'stay',\n",
       "  '.'],\n",
       " ['Here', 'are', 'a', 'few', 'good', 'links', 'for', 'newcomers', ':'],\n",
       " ['The', 'five', 'pillars', 'of', 'Wikipedia'],\n",
       " ['How', 'to', 'edit', 'a', 'page'],\n",
       " ['Help', 'pages'],\n",
       " ['Tutorial'],\n",
       " ['How', 'to', 'write', 'a', 'great', 'article']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we need to split each line into tokens. We can do this using the `split()` method, which splits a string into a list of words based on whitespace.\n",
    "# We will also remove any leading or trailing whitespace from each line.\n",
    "\n",
    "tokenized_data = []\n",
    "for line in data:\n",
    "    # Strip leading/trailing whitespace and split by whitespace\n",
    "    tokens = line.strip().split()\n",
    "    tokenized_data.append(tokens)\n",
    "\n",
    "# Alternatively, we could use a list comprehension to achieve the same result in a more compact way.\n",
    "# tokenized_data = [line.strip().split() for line in data]\n",
    "\n",
    "# Now, let's print the first 10 lines again to see the tokenized data.\n",
    "tokenized_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a884d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to import the Gensim library to create a Word2Vec model using the tokenized data.\n",
    "from gensim.models import Word2Vec\n",
    "# We will create a Word2Vec model using the CBOW approach. The parameters are:\n",
    "# - `vector_size`: The size of the word vectors (we will use 200 dimensions).\n",
    "# - `window`: The maximum distance between the current and predicted word within a sentence (we will use a window of 5).\n",
    "# - `min_count`: Ignores all words with total frequency lower than this (we will use 5).\n",
    "# - `workers`: The number of worker threads to train the model (we will use all available cores except 1).\n",
    "# - `sg`: Skip-gram model (1) or CBOW (0). We will use CBOW (0).\n",
    "\n",
    "# get available cores\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() - 1\n",
    "if cores > 50:\n",
    "    cores = 20\n",
    "\n",
    "# Create the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=200, window=5, min_count=5, workers=cores, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5005c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also save the model to a file for later use.\n",
    "model.save(\"data/wue15_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c98466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model later, we can use the following command:\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"data/wue15_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba16274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interesting part of the model is the `wv` attribute, which contains the word vectors.\n",
    "model = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8df79ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attacks', 0.6789256930351257),\n",
       " ('insult', 0.6435273289680481),\n",
       " ('vendetta', 0.5984145402908325),\n",
       " ('affront', 0.5594542026519775),\n",
       " ('vendettas', 0.5416612029075623),\n",
       " ('attacks.--', 0.5324352383613586),\n",
       " ('crusade', 0.5312038660049438),\n",
       " ('insults', 0.5214263200759888),\n",
       " ('slur', 0.5176430940628052),\n",
       " ('accusation', 0.4990154802799225),\n",
       " ('insulting', 0.4841175675392151),\n",
       " ('threat', 0.4821915626525879),\n",
       " ('harassment', 0.4795363247394562),\n",
       " ('grudge', 0.4791853725910187),\n",
       " ('observation', 0.47662192583084106),\n",
       " ('provocation', 0.47645336389541626),\n",
       " ('ad', 0.4752565324306488),\n",
       " ('attacking', 0.4742068350315094),\n",
       " ('slander', 0.45394542813301086),\n",
       " ('attack.--', 0.4515107572078705)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if it worked by asking for nearest neighbors of a word.\n",
    "model.most_similar(\"attack\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89af5f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 115621\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many words are in the vocabulary (= how many types from our training data occur at least 5 times in the corpus).\n",
    "print(f\"Vocabulary size: {len(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b897c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besides , I have a vendetta against forced antidepressants .\n",
      "You are now conducting a personal vendetta against me. You've decided to remove every single edit I've made to Trek pages .\n",
      "-- 15:12 , 18 December 2014 ( UTC ) Your AIV report I removed it. There is no \" vendetta \" , and if you're filing a complaint for edit warring you should have reported the other party as well .\n",
      "Should you come to your senses and decide to make constructive edits again , I don't think we'll have a problem , but if you insist on continuing your vendetta against the Powerpuff Girls , It won't end well .\n",
      "With no edits to his account there's no way to tell if it's tCv , this impersonator , someone with a vendetta against them , or just another vandal .\n",
      "Admit that you have a personal vendetta against this person .\n",
      "I just can't understand why SkyWriter has such a vendetta to push \" clarifying \" the article as \" Christian \" at the expense of marginalization of many Messianic Jews who reject the label not only nominally , but behaviorally and doctrinally as well .\n",
      "Please , if you have something personal against me , get over it and lay off. You seem to have a vendetta to solve and , if the world hasn't given you hints yet , I'll tell you flat out ...\n",
      "It seems like this guy has a vendetta against JLC .\n",
      "This could be construed as a personal vendetta on your part .\n"
     ]
    }
   ],
   "source": [
    "max_sents = 10\n",
    "printed = 0\n",
    "word = \"vendetta\"\n",
    "for sent in tokenized_data:\n",
    "    if word in sent:\n",
    "        print(\" \".join(sent))\n",
    "        printed += 1\n",
    "    if printed >= max_sents:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26b9f6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attacks', 0.9274438619613647),\n",
       " ('bombing', 0.8695274591445923),\n",
       " ('suicide', 0.8600563406944275),\n",
       " ('raid', 0.8567200303077698),\n",
       " ('bomb', 0.8250047564506531),\n",
       " ('ambush', 0.8242325782775879),\n",
       " ('killing', 0.8198403120040894),\n",
       " ('deadly', 0.8161444067955017),\n",
       " ('strikes', 0.8123002648353577),\n",
       " ('militants', 0.8120480179786682)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now compare the model to a pre-trained one.\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "contrast_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "contrast_model.most_similar(\"attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "992bc829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector 1: [-1.1933035   1.6471744   0.92219937 -1.18339     1.0685806   0.96526486\n",
      "  0.3699839  -0.4874719   0.22341654 -0.86063737]\n",
      "Vector 2: [ 1.4703  -0.9337   0.51369 -0.19082  0.50227  0.13241  0.12726  0.63662\n",
      " -0.13905 -0.32585]\n"
     ]
    }
   ],
   "source": [
    "# Importantly, the vectors of the two models are not directly comparable, as they are trained independently.\n",
    "\n",
    "vector1 = model[\"attack\"]\n",
    "vector2 = contrast_model[\"attack\"]\n",
    "print(f\"Vector 1: {vector1[:10]}\")\n",
    "print(f\"Vector 2: {vector2[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7605c0d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,) (50,) (200,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This will not work, as the vectors are not of the same size.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# And even if they were, the nearest neighbours would not be meaningful as the models are independently trained.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/gensim/models/keyedvectors.py:511\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(keys):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, ndarray):\n\u001b[0;32m--> 511\u001b[0m         mean \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weights[idx] \u001b[38;5;241m*\u001b[39m key\n\u001b[1;32m    512\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(key):\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (200,) (50,) (200,) "
     ]
    }
   ],
   "source": [
    "# This will not work, as the vectors are not of the same size.\n",
    "# And even if they were, the nearest neighbours would not be meaningful as the models are independently trained.\n",
    "model.most_similar(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c60db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attacks',\n",
       " 'insult',\n",
       " 'vendetta',\n",
       " 'affront',\n",
       " 'vendettas',\n",
       " 'attacks.--',\n",
       " 'crusade',\n",
       " 'insults',\n",
       " 'slur',\n",
       " 'accusation',\n",
       " 'insulting',\n",
       " 'threat',\n",
       " 'harassment',\n",
       " 'grudge',\n",
       " 'observation',\n",
       " 'provocation',\n",
       " 'ad',\n",
       " 'attacking',\n",
       " 'slander',\n",
       " 'attack.--']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What you can do, however, is to contrast the two models by contrasting the nearest neighbours of a word.\n",
    "\n",
    "attack1 = [word_score[0] for word_score in model.most_similar(\"attack\", topn=20)]\n",
    "attack2 = [word_score[0] for word_score in contrast_model.most_similar(\"attack\", topn=20)]\n",
    "\n",
    "attack1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a387f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word insult is in model 1 but not in model 2\n",
      "Word vendetta is in model 1 but not in model 2\n",
      "Word affront is in model 1 but not in model 2\n",
      "Word vendettas is in model 1 but not in model 2\n",
      "Word attacks.-- is in model 1 but not in model 2\n",
      "Word crusade is in model 1 but not in model 2\n",
      "Word insults is in model 1 but not in model 2\n",
      "Word slur is in model 1 but not in model 2\n",
      "Word accusation is in model 1 but not in model 2\n",
      "Word insulting is in model 1 but not in model 2\n",
      "Word threat is in model 1 but not in model 2\n",
      "Word harassment is in model 1 but not in model 2\n",
      "Word grudge is in model 1 but not in model 2\n",
      "Word observation is in model 1 but not in model 2\n",
      "Word provocation is in model 1 but not in model 2\n",
      "Word ad is in model 1 but not in model 2\n",
      "Word attacking is in model 1 but not in model 2\n",
      "Word slander is in model 1 but not in model 2\n",
      "Word attack.-- is in model 1 but not in model 2\n"
     ]
    }
   ],
   "source": [
    "for word in attack1:\n",
    "    if word not in attack2:\n",
    "        print(f\"Word {word} is in model 1 but not in model 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b63fd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following function that should return a percentage of overlapping words between two lists of words.\n",
    "def list_overlap(list1, list2):\n",
    "    # ... your code here\n",
    "    return overlap_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dab7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now integrate the previous steps and your list_overlap function into the function below.\n",
    "# It should take two words and two models as input and return the percentage of overlapping words among the n nearest neighbours of the words between the two models.\n",
    "\n",
    "def overlap_percentage(word, model1, model2, n=20):\n",
    "    # ... your code here    \n",
    "    return overlap_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f0fd56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', 'to', 'you', '(', ')', 'and', 'of', 'a']\n",
      "Shared vocabulary size: 41054\n"
     ]
    }
   ],
   "source": [
    "# Now we can go from a corpus-based to a corpus-driven perspective and look at all words in the vocabulary.\n",
    "\n",
    "# We find the vocab of the model by using the `index_to_key` attribute.\n",
    "print(model.index_to_key[:10])\n",
    "\n",
    "# We build a set of words that are shared between the two models.\n",
    "\n",
    "shared_vocab = set(model.index_to_key).intersection(set(contrast_model.index_to_key))\n",
    "print(f\"Shared vocabulary size: {len(shared_vocab)}\")\n",
    "\n",
    "# Now we can iterate over the vocabulary and save the overlap percentages in a dictionary.\n",
    "overlap_dict = {}\n",
    "\n",
    "for word in shared_vocab:\n",
    "    overlap = overlap_percentage(word, model, contrast_model)\n",
    "    overlap_dict[word] = overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "581ee010",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'function' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msort_dict\u001b[39m(dic, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28msorted\u001b[39m(dic\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39mreverse))\n\u001b[0;32m----> 6\u001b[0m sorted_overlap \u001b[38;5;241m=\u001b[39m \u001b[43msort_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverlap_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m, in \u001b[0;36msort_dict\u001b[0;34m(dic, reverse)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msort_dict\u001b[39m(dic, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'function' and 'function'"
     ]
    }
   ],
   "source": [
    "# Now we can sort the dictionary with this function\n",
    "\n",
    "def sort_dict(dic, reverse=True):\n",
    "    return dict(sorted(dic.items(), key=lambda item: item[1], reverse=reverse))\n",
    "\n",
    "sorted_overlap = sort_dict(overlap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a394d0b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_overlap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msorted_overlap\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_overlap' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "078954d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'function' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msort_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverlap_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m, in \u001b[0;36msort_dict\u001b[0;34m(dic, reverse)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msort_dict\u001b[39m(dic, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'function' and 'function'"
     ]
    }
   ],
   "source": [
    "sort_dict(overlap_dict, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b981801b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('386', 0.8149140477180481),\n",
       " ('SynaptiCAD', 0.8127642869949341),\n",
       " ('Epilepsy', 0.8126484155654907),\n",
       " ('Barros', 0.8122938275337219),\n",
       " ('waller', 0.810689389705658),\n",
       " ('Paco', 0.8102414608001709),\n",
       " ('Alemayehu', 0.8082616329193115),\n",
       " ('Haller', 0.8078862428665161),\n",
       " ('Chanderdat', 0.8072391748428345),\n",
       " ('Ele', 0.8070787787437439)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"spongebob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b03f719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('squarepants', 0.9683283567428589),\n",
       " ('buffy', 0.6830703616142273),\n",
       " ('nickelodeon', 0.6683040857315063),\n",
       " ('degeneres', 0.6681976914405823),\n",
       " ('cbbc', 0.666020929813385),\n",
       " ('veggietales', 0.6627994179725647),\n",
       " ('tigger', 0.6526259779930115),\n",
       " ('roseanne', 0.6281614303588867),\n",
       " ('shrek', 0.6193712949752808),\n",
       " ('whisperer', 0.6192430257797241)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_model.most_similar(\"spongebob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0653f92-f288-4564-91a6-c7b717dd0d74",
   "metadata": {},
   "source": [
    "# Original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68496908-8234-4cf6-b8ac-abdff320500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model later, we can use the following command:\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"data_ori/wue15_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3836b481-89fc-431e-8325-7e00b8399120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interesting part of the model is the `wv` attribute, which contains the word vectors.\n",
    "model = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9b66e3c-2585-46a4-8032-0ddbedcea8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attacks', 0.6653181910514832),\n",
       " ('insult', 0.6299493312835693),\n",
       " ('vendetta', 0.627418041229248),\n",
       " ('attack.--', 0.6023419499397278),\n",
       " ('slur', 0.5512348413467407),\n",
       " ('crusade', 0.5488301515579224),\n",
       " ('affront', 0.5374261736869812),\n",
       " ('vendettas', 0.5121461153030396),\n",
       " ('accusation', 0.5028104186058044),\n",
       " ('attacks.--', 0.5013230443000793),\n",
       " ('slander', 0.5011648535728455),\n",
       " ('grudge', 0.5004501342773438),\n",
       " ('threat', 0.494328111410141),\n",
       " ('insults', 0.49367964267730713),\n",
       " ('allegation', 0.49109581112861633),\n",
       " ('harassment', 0.49007099866867065),\n",
       " ('ad', 0.48044106364250183),\n",
       " ('atacks', 0.4773373603820801),\n",
       " ('observation', 0.4734446406364441),\n",
       " ('insulting', 0.4729105234146118)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if it worked by asking for nearest neighbors of a word.\n",
    "model.most_similar(\"attack\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58c732ba-16f2-4c5b-9c52-829e7ee84a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 115621\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many words are in the vocabulary (= how many types from our training data occur at least 5 times in the corpus).\n",
    "print(f\"Vocabulary size: {len(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1931ef4e-1448-43fc-b2f9-2b3a9102d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besides , I have a vendetta against forced antidepressants .\n",
      "You are now conducting a personal vendetta against me. You've decided to remove every single edit I've made to Trek pages .\n",
      "-- 15:12 , 18 December 2014 ( UTC ) Your AIV report I removed it. There is no \" vendetta \" , and if you're filing a complaint for edit warring you should have reported the other party as well .\n",
      "Should you come to your senses and decide to make constructive edits again , I don't think we'll have a problem , but if you insist on continuing your vendetta against the Powerpuff Girls , It won't end well .\n",
      "With no edits to his account there's no way to tell if it's tCv , this impersonator , someone with a vendetta against them , or just another vandal .\n",
      "Admit that you have a personal vendetta against this person .\n",
      "I just can't understand why SkyWriter has such a vendetta to push \" clarifying \" the article as \" Christian \" at the expense of marginalization of many Messianic Jews who reject the label not only nominally , but behaviorally and doctrinally as well .\n",
      "Please , if you have something personal against me , get over it and lay off. You seem to have a vendetta to solve and , if the world hasn't given you hints yet , I'll tell you flat out ...\n",
      "It seems like this guy has a vendetta against JLC .\n",
      "This could be construed as a personal vendetta on your part .\n"
     ]
    }
   ],
   "source": [
    "max_sents = 10\n",
    "printed = 0\n",
    "word = \"vendetta\"\n",
    "for sent in tokenized_data:\n",
    "    if word in sent:\n",
    "        print(\" \".join(sent))\n",
    "        printed += 1\n",
    "    if printed >= max_sents:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d00a2b1-833f-48b3-b023-3432c5fcdc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attacks', 0.9274438619613647),\n",
       " ('bombing', 0.8695274591445923),\n",
       " ('suicide', 0.8600563406944275),\n",
       " ('raid', 0.8567200303077698),\n",
       " ('bomb', 0.8250047564506531),\n",
       " ('ambush', 0.8242325782775879),\n",
       " ('killing', 0.8198403120040894),\n",
       " ('deadly', 0.8161444067955017),\n",
       " ('strikes', 0.8123002648353577),\n",
       " ('militants', 0.8120480179786682)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now compare the model to a pre-trained one.\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "contrast_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "contrast_model.most_similar(\"attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ac51a0e-d707-4d2f-b60e-3c267b34d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector 1: [ 0.80315816  1.2910506  -0.90030617  0.81309754  0.18535283  1.4075223\n",
      " -0.75989944 -1.7453072   0.42122108 -1.1356882 ]\n",
      "Vector 2: [ 1.4703  -0.9337   0.51369 -0.19082  0.50227  0.13241  0.12726  0.63662\n",
      " -0.13905 -0.32585]\n"
     ]
    }
   ],
   "source": [
    "# Importantly, the vectors of the two models are not directly comparable, as they are trained independently.\n",
    "\n",
    "vector1 = model[\"attack\"]\n",
    "vector2 = contrast_model[\"attack\"]\n",
    "print(f\"Vector 1: {vector1[:10]}\")\n",
    "print(f\"Vector 2: {vector2[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "844f531f-eadb-49d6-bbbd-ebccf48672e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,) (50,) (200,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This will not work, as the vectors are not of the same size.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# And even if they were, the nearest neighbours would not be meaningful as the models are independently trained.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/gensim/models/keyedvectors.py:511\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(keys):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, ndarray):\n\u001b[0;32m--> 511\u001b[0m         mean \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weights[idx] \u001b[38;5;241m*\u001b[39m key\n\u001b[1;32m    512\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(key):\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (200,) (50,) (200,) "
     ]
    }
   ],
   "source": [
    "# This will not work, as the vectors are not of the same size.\n",
    "# And even if they were, the nearest neighbours would not be meaningful as the models are independently trained.\n",
    "model.most_similar(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cb6327b-6dd2-4110-92ac-c2d882c85045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attacks',\n",
       " 'insult',\n",
       " 'vendetta',\n",
       " 'attack.--',\n",
       " 'slur',\n",
       " 'crusade',\n",
       " 'affront',\n",
       " 'vendettas',\n",
       " 'accusation',\n",
       " 'attacks.--',\n",
       " 'slander',\n",
       " 'grudge',\n",
       " 'threat',\n",
       " 'insults',\n",
       " 'allegation',\n",
       " 'harassment',\n",
       " 'ad',\n",
       " 'atacks',\n",
       " 'observation',\n",
       " 'insulting']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What you can do, however, is to contrast the two models by contrasting the nearest neighbours of a word.\n",
    "\n",
    "attack1 = [word_score[0] for word_score in model.most_similar(\"attack\", topn=20)]\n",
    "attack2 = [word_score[0] for word_score in contrast_model.most_similar(\"attack\", topn=20)]\n",
    "\n",
    "attack1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9fd12bd-7e66-45f8-a184-2b549373b773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word insult is in model 1 but not in model 2\n",
      "Word vendetta is in model 1 but not in model 2\n",
      "Word attack.-- is in model 1 but not in model 2\n",
      "Word slur is in model 1 but not in model 2\n",
      "Word crusade is in model 1 but not in model 2\n",
      "Word affront is in model 1 but not in model 2\n",
      "Word vendettas is in model 1 but not in model 2\n",
      "Word accusation is in model 1 but not in model 2\n",
      "Word attacks.-- is in model 1 but not in model 2\n",
      "Word slander is in model 1 but not in model 2\n",
      "Word grudge is in model 1 but not in model 2\n",
      "Word threat is in model 1 but not in model 2\n",
      "Word insults is in model 1 but not in model 2\n",
      "Word allegation is in model 1 but not in model 2\n",
      "Word harassment is in model 1 but not in model 2\n",
      "Word ad is in model 1 but not in model 2\n",
      "Word atacks is in model 1 but not in model 2\n",
      "Word observation is in model 1 but not in model 2\n",
      "Word insulting is in model 1 but not in model 2\n"
     ]
    }
   ],
   "source": [
    "for word in attack1:\n",
    "    if word not in attack2:\n",
    "        print(f\"Word {word} is in model 1 but not in model 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6220dfa1-1b3e-4d95-bb86-45486b4b9a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following function that should return a percentage of overlapping words between two lists of words.\n",
    "def list_overlap(list1, list2):\n",
    "    # ... your code here\n",
    "    return overlap_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0baa219-e420-4aa8-b128-d649696c54c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now integrate the previous steps and your list_overlap function into the function below.\n",
    "# It should take two words and two models as input and return the percentage of overlapping words among the n nearest neighbours of the words between the two models.\n",
    "\n",
    "def overlap_percentage(word, model1, model2, n=20):\n",
    "    # ... your code here    \n",
    "    return overlap_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6912fe71-1a01-495b-b91b-0d5570865246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', 'to', 'you', '(', ')', 'and', 'of', 'a']\n",
      "Shared vocabulary size: 41054\n"
     ]
    }
   ],
   "source": [
    "# Now we can go from a corpus-based to a corpus-driven perspective and look at all words in the vocabulary.\n",
    "\n",
    "# We find the vocab of the model by using the `index_to_key` attribute.\n",
    "print(model.index_to_key[:10])\n",
    "\n",
    "# We build a set of words that are shared between the two models.\n",
    "\n",
    "shared_vocab = set(model.index_to_key).intersection(set(contrast_model.index_to_key))\n",
    "print(f\"Shared vocabulary size: {len(shared_vocab)}\")\n",
    "\n",
    "# Now we can iterate over the vocabulary and save the overlap percentages in a dictionary.\n",
    "overlap_dict = {}\n",
    "\n",
    "for word in shared_vocab:\n",
    "    overlap = overlap_percentage(word, model, contrast_model)\n",
    "    overlap_dict[word] = overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fb3407d-49c5-4d63-92fe-01e1382ed599",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'function' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msort_dict\u001b[39m(dic, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28msorted\u001b[39m(dic\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39mreverse))\n\u001b[0;32m----> 6\u001b[0m sorted_overlap \u001b[38;5;241m=\u001b[39m \u001b[43msort_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverlap_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m, in \u001b[0;36msort_dict\u001b[0;34m(dic, reverse)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msort_dict\u001b[39m(dic, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'function' and 'function'"
     ]
    }
   ],
   "source": [
    "# Now we can sort the dictionary with this function\n",
    "\n",
    "def sort_dict(dic, reverse=True):\n",
    "    return dict(sorted(dic.items(), key=lambda item: item[1], reverse=reverse))\n",
    "\n",
    "sorted_overlap = sort_dict(overlap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "728c4919-d237-4a5a-bf55-77234b3e9470",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_overlap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msorted_overlap\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_overlap' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff25f93d-5652-4438-b442-59c3854a940f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'function' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msort_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverlap_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m, in \u001b[0;36msort_dict\u001b[0;34m(dic, reverse)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msort_dict\u001b[39m(dic, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'function' and 'function'"
     ]
    }
   ],
   "source": [
    "sort_dict(overlap_dict, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e01b528-ab21-4bb6-adde-7ac2d94bbd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Din-e', 0.7905212640762329),\n",
       " ('Ujfaluši', 0.7738329768180847),\n",
       " ('Rikki', 0.771855890750885),\n",
       " ('chính', 0.770332396030426),\n",
       " ('kaul', 0.7690215110778809),\n",
       " ('Haller', 0.768900990486145),\n",
       " ('costumbre', 0.7681359648704529),\n",
       " ('Branders.com', 0.7681124806404114),\n",
       " ('Velázquez', 0.7669525146484375),\n",
       " ('Voskhod', 0.7669331431388855)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"spongebob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2906433d-2175-4b6c-abef-dd6a6480a9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('squarepants', 0.9683283567428589),\n",
       " ('buffy', 0.6830703616142273),\n",
       " ('nickelodeon', 0.6683040857315063),\n",
       " ('degeneres', 0.6681976914405823),\n",
       " ('cbbc', 0.666020929813385),\n",
       " ('veggietales', 0.6627994179725647),\n",
       " ('tigger', 0.6526259779930115),\n",
       " ('roseanne', 0.6281614303588867),\n",
       " ('shrek', 0.6193712949752808),\n",
       " ('whisperer', 0.6192430257797241)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_model.most_similar(\"spongebob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d3a0b-4f81-40a5-8060-e0ed2c9c7af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
