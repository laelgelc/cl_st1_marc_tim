{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data (we assume that we have one sentence per line and that the tokens are separated by whitespace)\n",
    "\n",
    "with open(\"wue15.txt\", \"r\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c113827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the data is a list of strings, where each string is a line from the file.\n",
    "# Let's print the first 10 lines to see what we have.\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83884de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to split each line into tokens. We can do this using the `split()` method, which splits a string into a list of words based on whitespace.\n",
    "# We will also remove any leading or trailing whitespace from each line.\n",
    "\n",
    "tokenized_data = []\n",
    "for line in data:\n",
    "    # Strip leading/trailing whitespace and split by whitespace\n",
    "    tokens = line.strip().split()\n",
    "    tokenized_data.append(tokens)\n",
    "\n",
    "# Alternatively, we could use a list comprehension to achieve the same result in a more compact way.\n",
    "# tokenized_data = [line.strip().split() for line in data]\n",
    "\n",
    "# Now, let's print the first 10 lines again to see the tokenized data.\n",
    "tokenized_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a884d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to import the Gensim library to create a Word2Vec model using the tokenized data.\n",
    "from gensim.models import Word2Vec\n",
    "# We will create a Word2Vec model using the CBOW approach. The parameters are:\n",
    "# - `vector_size`: The size of the word vectors (we will use 200 dimensions).\n",
    "# - `window`: The maximum distance between the current and predicted word within a sentence (we will use a window of 5).\n",
    "# - `min_count`: Ignores all words with total frequency lower than this (we will use 5).\n",
    "# - `workers`: The number of worker threads to train the model (we will use all available cores except 1).\n",
    "# - `sg`: Skip-gram model (1) or CBOW (0). We will use CBOW (0).\n",
    "\n",
    "# get available cores\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() - 1\n",
    "if cores > 50:\n",
    "    cores = 20\n",
    "\n",
    "# Create the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=200, window=5, min_count=5, workers=cores, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also save the model to a file for later use.\n",
    "model.save(\"wue15_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c98466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model later, we can use the following command:\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"wue15_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba16274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interesting part of the model is the `wv` attribute, which contains the word vectors.\n",
    "model = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if it worked by asking for nearest neighbors of a word.\n",
    "model.most_similar(\"attack\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many words are in the vocabulary (= how many types from our training data occur at least 5 times in the corpus).\n",
    "print(f\"Vocabulary size: {len(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sents = 10\n",
    "printed = 0\n",
    "word = \"vendetta\"\n",
    "for sent in tokenized_data:\n",
    "    if word in sent:\n",
    "        print(\" \".join(sent))\n",
    "        printed += 1\n",
    "    if printed >= max_sents:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now compare the model to a pre-trained one.\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "contrast_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "contrast_model.most_similar(\"attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importantly, the vectors of the two models are not directly comparable, as they are trained independently.\n",
    "\n",
    "vector1 = model[\"attack\"]\n",
    "vector2 = contrast_model[\"attack\"]\n",
    "print(f\"Vector 1: {vector1[:10]}\")\n",
    "print(f\"Vector 2: {vector2[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will not work, as the vectors are not of the same size.\n",
    "# And even if they were, the nearest neighbours would not be meaningful as the models are independently trained.\n",
    "model.most_similar(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c60db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What you can do, however, is to contrast the two models by contrasting the nearest neighbours of a word.\n",
    "\n",
    "attack1 = [word_score[0] for word_score in model.most_similar(\"attack\", topn=20)]\n",
    "attack2 = [word_score[0] for word_score in contrast_model.most_similar(\"attack\", topn=20)]\n",
    "\n",
    "attack1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a387f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in attack1:\n",
    "    if word not in attack2:\n",
    "        print(f\"Word {word} is in model 1 but not in model 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following function that should return a percentage of overlapping words between two lists of words.\n",
    "def list_overlap(list1, list2):\n",
    "    # ... your code here\n",
    "    return overlap_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now integrate the previous steps and your list_overlap function into the function below.\n",
    "# It should take two words and two models as input and return the percentage of overlapping words among the n nearest neighbours of the words between the two models.\n",
    "\n",
    "def overlap_percentage(word, model1, model2, n=20):\n",
    "    # ... your code here    \n",
    "    return overlap_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fd56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can go from a corpus-based to a corpus-driven perspective and look at all words in the vocabulary.\n",
    "\n",
    "# We find the vocab of the model by using the `index_to_key` attribute.\n",
    "print(model.index_to_key[:10])\n",
    "\n",
    "# We build a set of words that are shared between the two models.\n",
    "\n",
    "shared_vocab = set(model.index_to_key).intersection(set(contrast_model.index_to_key))\n",
    "print(f\"Shared vocabulary size: {len(shared_vocab)}\")\n",
    "\n",
    "# Now we can iterate over the vocabulary and save the overlap percentages in a dictionary.\n",
    "overlap_dict = {}\n",
    "\n",
    "for word in shared_vocab:\n",
    "    overlap = overlap_percentage(word, model, contrast_model)\n",
    "    overlap_dict[word] = overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ee010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can sort the dictionary with this function\n",
    "\n",
    "def sort_dict(dic, reverse=True):\n",
    "    return dict(sorted(dic.items(), key=lambda item: item[1], reverse=reverse))\n",
    "\n",
    "sorted_overlap = sort_dict(overlap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078954d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_dict(overlap_dict, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"spongebob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_model.most_similar(\"spongebob\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_course_word_embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
