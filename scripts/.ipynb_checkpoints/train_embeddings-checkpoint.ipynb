{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0dbda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data (we assume that we have one sentence per line and that the tokens are separated by whitespace)\n",
    "\n",
    "with open(\"data/wue15.txt\", \"r\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c113827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome !\\n',\n",
       " 'Hello , , and welcome to Wikipedia !\\n',\n",
       " 'Thank you for your contributions .\\n',\n",
       " 'I hope you like the place and decide to stay .\\n',\n",
       " 'Here are a few good links for newcomers :\\n',\n",
       " 'The five pillars of Wikipedia\\n',\n",
       " 'How to edit a page\\n',\n",
       " 'Help pages\\n',\n",
       " 'Tutorial\\n',\n",
       " 'How to write a great article\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see, the data is a list of strings, where each string is a line from the file.\n",
    "# Let's print the first 10 lines to see what we have.\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83884de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Welcome', '!'],\n",
       " ['Hello', ',', ',', 'and', 'welcome', 'to', 'Wikipedia', '!'],\n",
       " ['Thank', 'you', 'for', 'your', 'contributions', '.'],\n",
       " ['I',\n",
       "  'hope',\n",
       "  'you',\n",
       "  'like',\n",
       "  'the',\n",
       "  'place',\n",
       "  'and',\n",
       "  'decide',\n",
       "  'to',\n",
       "  'stay',\n",
       "  '.'],\n",
       " ['Here', 'are', 'a', 'few', 'good', 'links', 'for', 'newcomers', ':'],\n",
       " ['The', 'five', 'pillars', 'of', 'Wikipedia'],\n",
       " ['How', 'to', 'edit', 'a', 'page'],\n",
       " ['Help', 'pages'],\n",
       " ['Tutorial'],\n",
       " ['How', 'to', 'write', 'a', 'great', 'article']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we need to split each line into tokens. We can do this using the `split()` method, which splits a string into a list of words based on whitespace.\n",
    "# We will also remove any leading or trailing whitespace from each line.\n",
    "\n",
    "tokenized_data = []\n",
    "for line in data:\n",
    "    # Strip leading/trailing whitespace and split by whitespace\n",
    "    tokens = line.strip().split()\n",
    "    tokenized_data.append(tokens)\n",
    "\n",
    "# Alternatively, we could use a list comprehension to achieve the same result in a more compact way.\n",
    "# tokenized_data = [line.strip().split() for line in data]\n",
    "\n",
    "# Now, let's print the first 10 lines again to see the tokenized data.\n",
    "tokenized_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a884d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to import the Gensim library to create a Word2Vec model using the tokenized data.\n",
    "from gensim.models import Word2Vec\n",
    "# We will create a Word2Vec model using the CBOW approach. The parameters are:\n",
    "# - `vector_size`: The size of the word vectors (we will use 200 dimensions).\n",
    "# - `window`: The maximum distance between the current and predicted word within a sentence (we will use a window of 5).\n",
    "# - `min_count`: Ignores all words with total frequency lower than this (we will use 5).\n",
    "# - `workers`: The number of worker threads to train the model (we will use all available cores except 1).\n",
    "# - `sg`: Skip-gram model (1) or CBOW (0). We will use CBOW (0).\n",
    "\n",
    "# get available cores\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() - 1\n",
    "if cores > 50:\n",
    "    cores = 20\n",
    "\n",
    "# Create the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=200, window=5, min_count=5, workers=cores, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5005c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also save the model to a file for later use.\n",
    "model.save(\"data/wue15_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c98466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model later, we can use the following command:\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"data/wue15_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba16274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interesting part of the model is the `wv` attribute, which contains the word vectors.\n",
    "model = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8df79ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attacks', 0.6597480177879333),\n",
       " ('vendetta', 0.6423077583312988),\n",
       " ('insult', 0.6314612030982971),\n",
       " ('attack.--', 0.5956201553344727),\n",
       " ('affront', 0.5313739776611328),\n",
       " ('crusade', 0.5311992168426514),\n",
       " ('attacks.--', 0.5286079049110413),\n",
       " ('vendettas', 0.5196763277053833),\n",
       " ('accusation', 0.5169777870178223),\n",
       " ('harassment', 0.5155003666877747),\n",
       " ('insults', 0.5018538236618042),\n",
       " ('allegation', 0.49789944291114807),\n",
       " ('slur', 0.49663016200065613),\n",
       " ('threat', 0.49402889609336853),\n",
       " ('grudge', 0.4926571547985077),\n",
       " ('slander', 0.4887342154979706),\n",
       " ('animus', 0.48533332347869873),\n",
       " ('attacking', 0.4831395149230957),\n",
       " ('insulting', 0.47360464930534363),\n",
       " ('observation', 0.4726438522338867)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if it worked by asking for nearest neighbors of a word.\n",
    "model.most_similar(\"attack\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89af5f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 115621\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many words are in the vocabulary (= how many types from our training data occur at least 5 times in the corpus).\n",
    "print(f\"Vocabulary size: {len(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b897c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besides , I have a vendetta against forced antidepressants .\n",
      "You are now conducting a personal vendetta against me. You've decided to remove every single edit I've made to Trek pages .\n",
      "-- 15:12 , 18 December 2014 ( UTC ) Your AIV report I removed it. There is no \" vendetta \" , and if you're filing a complaint for edit warring you should have reported the other party as well .\n",
      "Should you come to your senses and decide to make constructive edits again , I don't think we'll have a problem , but if you insist on continuing your vendetta against the Powerpuff Girls , It won't end well .\n",
      "With no edits to his account there's no way to tell if it's tCv , this impersonator , someone with a vendetta against them , or just another vandal .\n",
      "Admit that you have a personal vendetta against this person .\n",
      "I just can't understand why SkyWriter has such a vendetta to push \" clarifying \" the article as \" Christian \" at the expense of marginalization of many Messianic Jews who reject the label not only nominally , but behaviorally and doctrinally as well .\n",
      "Please , if you have something personal against me , get over it and lay off. You seem to have a vendetta to solve and , if the world hasn't given you hints yet , I'll tell you flat out ...\n",
      "It seems like this guy has a vendetta against JLC .\n",
      "This could be construed as a personal vendetta on your part .\n"
     ]
    }
   ],
   "source": [
    "max_sents = 10\n",
    "printed = 0\n",
    "word = \"vendetta\"\n",
    "for sent in tokenized_data:\n",
    "    if word in sent:\n",
    "        print(\" \".join(sent))\n",
    "        printed += 1\n",
    "    if printed >= max_sents:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26b9f6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attacks', 0.9274438619613647),\n",
       " ('bombing', 0.8695275783538818),\n",
       " ('suicide', 0.8600562214851379),\n",
       " ('raid', 0.8567200303077698),\n",
       " ('bomb', 0.8250046968460083),\n",
       " ('ambush', 0.8242325186729431),\n",
       " ('killing', 0.8198404312133789),\n",
       " ('deadly', 0.8161443471908569),\n",
       " ('strikes', 0.8123002052307129),\n",
       " ('militants', 0.8120480179786682)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now compare the model to a pre-trained one.\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "contrast_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "contrast_model.most_similar(\"attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "992bc829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector 1: [ 0.21540718  0.41876966  1.1710385   0.33215466  0.79576355  0.7543912\n",
      " -1.326227   -0.42594796  1.5088825  -1.0433763 ]\n",
      "Vector 2: [ 1.4703  -0.9337   0.51369 -0.19082  0.50227  0.13241  0.12726  0.63662\n",
      " -0.13905 -0.32585]\n"
     ]
    }
   ],
   "source": [
    "# Importantly, the vectors of the two models are not directly comparable, as they are trained independently.\n",
    "\n",
    "vector1 = model[\"attack\"]\n",
    "vector2 = contrast_model[\"attack\"]\n",
    "print(f\"Vector 1: {vector1[:10]}\")\n",
    "print(f\"Vector 2: {vector2[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7605c0d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,) (50,) (200,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This will not work, as the vectors are not of the same size.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# And even if they were, the nearest neighbours would not be meaningful as the models are independently trained.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my_env/lib/python3.12/site-packages/gensim/models/keyedvectors.py:841\u001b[39m, in \u001b[36mKeyedVectors.most_similar\u001b[39m\u001b[34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[39m\n\u001b[32m    838\u001b[39m         weight[idx] = item[\u001b[32m1\u001b[39m]\n\u001b[32m    840\u001b[39m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m mean = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m all_keys = [\n\u001b[32m    843\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_index_for(key)\n\u001b[32m    844\u001b[39m ]\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my_env/lib/python3.12/site-packages/gensim/models/keyedvectors.py:511\u001b[39m, in \u001b[36mKeyedVectors.get_mean_vector\u001b[39m\u001b[34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(keys):\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, ndarray):\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         \u001b[43mmean\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\n\u001b[32m    512\u001b[39m         total_weight += \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__contains__\u001b[39m(key):\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (200,) (50,) (200,) "
     ]
    }
   ],
   "source": [
    "# This will not work, as the vectors are not of the same size.\n",
    "# And even if they were, the nearest neighbours would not be meaningful as the models are independently trained.\n",
    "model.most_similar(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c60db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attacks',\n",
       " 'vendetta',\n",
       " 'insult',\n",
       " 'attack.--',\n",
       " 'affront',\n",
       " 'crusade',\n",
       " 'attacks.--',\n",
       " 'vendettas',\n",
       " 'accusation',\n",
       " 'harassment',\n",
       " 'insults',\n",
       " 'allegation',\n",
       " 'slur',\n",
       " 'threat',\n",
       " 'grudge',\n",
       " 'slander',\n",
       " 'animus',\n",
       " 'attacking',\n",
       " 'insulting',\n",
       " 'observation']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What you can do, however, is to contrast the two models by contrasting the nearest neighbours of a word.\n",
    "\n",
    "attack1 = [word_score[0] for word_score in model.most_similar(\"attack\", topn=20)]\n",
    "attack2 = [word_score[0] for word_score in contrast_model.most_similar(\"attack\", topn=20)]\n",
    "\n",
    "attack1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a387f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vendetta is in model 1 but not in model 2\n",
      "Word insult is in model 1 but not in model 2\n",
      "Word attack.-- is in model 1 but not in model 2\n",
      "Word affront is in model 1 but not in model 2\n",
      "Word crusade is in model 1 but not in model 2\n",
      "Word attacks.-- is in model 1 but not in model 2\n",
      "Word vendettas is in model 1 but not in model 2\n",
      "Word accusation is in model 1 but not in model 2\n",
      "Word harassment is in model 1 but not in model 2\n",
      "Word insults is in model 1 but not in model 2\n",
      "Word allegation is in model 1 but not in model 2\n",
      "Word slur is in model 1 but not in model 2\n",
      "Word threat is in model 1 but not in model 2\n",
      "Word grudge is in model 1 but not in model 2\n",
      "Word slander is in model 1 but not in model 2\n",
      "Word animus is in model 1 but not in model 2\n",
      "Word attacking is in model 1 but not in model 2\n",
      "Word insulting is in model 1 but not in model 2\n",
      "Word observation is in model 1 but not in model 2\n"
     ]
    }
   ],
   "source": [
    "for word in attack1:\n",
    "    if word not in attack2:\n",
    "        print(f\"Word {word} is in model 1 but not in model 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b63fd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following function that should return a percentage of overlapping words between two lists of words.\n",
    "def list_overlap(list1, list2):\n",
    "    # ... your code here\n",
    "    return overlap_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dab7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now integrate the previous steps and your list_overlap function into the function below.\n",
    "# It should take two words and two models as input and return the percentage of overlapping words among the n nearest neighbours of the words between the two models.\n",
    "\n",
    "def overlap_percentage(word, model1, model2, n=20):\n",
    "    # ... your code here    \n",
    "    return overlap_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f0fd56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', 'to', 'you', '(', ')', 'and', 'of', 'a']\n",
      "Shared vocabulary size: 41054\n"
     ]
    }
   ],
   "source": [
    "# Now we can go from a corpus-based to a corpus-driven perspective and look at all words in the vocabulary.\n",
    "\n",
    "# We find the vocab of the model by using the `index_to_key` attribute.\n",
    "print(model.index_to_key[:10])\n",
    "\n",
    "# We build a set of words that are shared between the two models.\n",
    "\n",
    "shared_vocab = set(model.index_to_key).intersection(set(contrast_model.index_to_key))\n",
    "print(f\"Shared vocabulary size: {len(shared_vocab)}\")\n",
    "\n",
    "# Now we can iterate over the vocabulary and save the overlap percentages in a dictionary.\n",
    "overlap_dict = {}\n",
    "\n",
    "for word in shared_vocab:\n",
    "    overlap = overlap_percentage(word, model, contrast_model)\n",
    "    overlap_dict[word] = overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "581ee010",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'function' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msort_dict\u001b[39m(dic, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28msorted\u001b[39m(dic.items(), key=\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[32m1\u001b[39m], reverse=reverse))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sorted_overlap = \u001b[43msort_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverlap_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36msort_dict\u001b[39m\u001b[34m(dic, reverse)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msort_dict\u001b[39m(dic, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'function' and 'function'"
     ]
    }
   ],
   "source": [
    "# Now we can sort the dictionary with this function\n",
    "\n",
    "def sort_dict(dic, reverse=True):\n",
    "    return dict(sorted(dic.items(), key=lambda item: item[1], reverse=reverse))\n",
    "\n",
    "sorted_overlap = sort_dict(overlap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a394d0b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_overlap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msorted_overlap\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'sorted_overlap' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "078954d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'function' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msort_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverlap_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36msort_dict\u001b[39m\u001b[34m(dic, reverse)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msort_dict\u001b[39m(dic, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'function' and 'function'"
     ]
    }
   ],
   "source": [
    "sort_dict(overlap_dict, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b981801b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Balducci', 0.7875739932060242),\n",
       " ('Mahdavi', 0.7848607301712036),\n",
       " ('Vihar', 0.7751529812812805),\n",
       " ('；', 0.7746995687484741),\n",
       " ('Rumah', 0.7742698192596436),\n",
       " ('Haller', 0.773401141166687),\n",
       " ('Augeant', 0.7723730802536011),\n",
       " ('Gurinder', 0.772068202495575),\n",
       " ('Ismat', 0.7718711495399475),\n",
       " ('romises', 0.7717167139053345)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"spongebob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b03f719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('squarepants', 0.9683282971382141),\n",
       " ('buffy', 0.6830703020095825),\n",
       " ('nickelodeon', 0.6683041453361511),\n",
       " ('degeneres', 0.6681976318359375),\n",
       " ('cbbc', 0.6660208702087402),\n",
       " ('veggietales', 0.6627993583679199),\n",
       " ('tigger', 0.6526259779930115),\n",
       " ('roseanne', 0.6281614303588867),\n",
       " ('shrek', 0.6193712949752808),\n",
       " ('whisperer', 0.6192430257797241)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_model.most_similar(\"spongebob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab24686-71ae-4f0c-af94-60e8ca2047a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
