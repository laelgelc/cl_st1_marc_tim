{"cells":[{"cell_type":"code","execution_count":1,"id":"3rPlTAvSOXa7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rPlTAvSOXa7","outputId":"5113f02c-fbc3-4b01-e943-44da78d6e3b8","executionInfo":{"status":"ok","timestamp":1747419997474,"user_tz":180,"elapsed":36498,"user":{"displayName":"Marc Kupietz","userId":"00093381237476284934"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m897.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]},{"output_type":"execute_result","data":{"text/plain":["{'status': 'ok', 'restart': True}"]},"metadata":{},"execution_count":1}],"source":["# Execute if you are working in colab\n","!pip install -qq numpy==1.26.4 gensim\n","get_ipython().kernel.do_shutdown (restart=True)"]},{"cell_type":"code","execution_count":1,"id":"c07763ea","metadata":{"id":"c07763ea","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1747420011136,"user_tz":180,"elapsed":4021,"user":{"displayName":"Marc Kupietz","userId":"00093381237476284934"}},"outputId":"cce20270-6f6d-49b6-8b63-1839f70c09e2"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'sample_data/wue15_word2vec.model'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dfa340be7ec9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_data/wue15_word2vec.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \"\"\"\n\u001b[0;32m-> 1459\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data/wue15_word2vec.model'"]}],"source":["# Let's now cluster our self-trained Wikipedia discussions model\n","from gensim.models import Word2Vec\n","\n","model = Word2Vec.load(\"sample_data/wue15_word2vec.model\")\n","model = model.wv"]},{"cell_type":"code","execution_count":null,"id":"d2db924c","metadata":{"id":"d2db924c"},"outputs":[],"source":["# Get the words and their vectors\n","words = model.index_to_key\n","word_vectors = model[words]\n","print(\"Number of vectors to be clustered:\", len(word_vectors))\n"]},{"cell_type":"code","execution_count":null,"id":"78d754c5","metadata":{"id":"78d754c5"},"outputs":[],"source":["from sklearn import cluster\n","# We will use the KMeans clustering algorithm\n","# We need to choose a number of clusters (k)\n","# We will not use too many clusters so it goes fast\n","k = 500\n","\n","# An alternative would be to take a certain percentage, e.g. 2.4 % of the number of types\n","# k = int(len(word_vectors) * 0.024)\n","\n","print(\"Number of clusters:\", k)\n","kmeans = cluster.KMeans(n_clusters=k)\n","kmeans.fit(word_vectors)"]},{"cell_type":"code","execution_count":null,"id":"842a2351","metadata":{"id":"842a2351"},"outputs":[],"source":["import pickle\n","# Save the model\n","with open(\"kmeans_wue15_word2vec.pkl\", \"wb\") as f:\n","    pickle.dump(kmeans, f)"]},{"cell_type":"code","execution_count":null,"id":"2655b221","metadata":{"id":"2655b221"},"outputs":[],"source":["import pickle\n","# Read the model\n","with open(\"kmeans_wue15_word2vec.pkl\", \"rb\") as f:\n","    kmeans = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"id":"ba29e53c","metadata":{"id":"ba29e53c"},"outputs":[],"source":["# The labels are the cluster IDs for each word\n","kmeans.labels_"]},{"cell_type":"code","execution_count":null,"id":"059f9175","metadata":{"id":"059f9175"},"outputs":[],"source":["# Let's create a dictionary to map words to their cluster IDs\n","\n","word_cluster = dict(zip(words, kmeans.labels_))\n","print(word_cluster)\n","\n","cluster_words = {}\n","for word, cluster in word_cluster.items():\n","    if cluster not in cluster_words:\n","        cluster_words[cluster] = []\n","    cluster_words[cluster].append(word)"]},{"cell_type":"code","execution_count":null,"id":"ec3b6b23","metadata":{"id":"ec3b6b23"},"outputs":[],"source":["import numpy as np\n","\n","cluster_centroid = {}\n","for cluster, words in cluster_words.items():\n","    vectors = [model[word] for word in words]\n","    cluster_centroid[cluster] = np.mean(vectors, axis=0)\n"]},{"cell_type":"code","execution_count":null,"id":"7e14e07d","metadata":{"id":"7e14e07d"},"outputs":[],"source":["from scipy.spatial import distance\n","import pandas as pd\n","\n","# Create DataFrame for cluster assignment and similarity\n","df = pd.DataFrame(word_cluster.items(), columns=[\"word\", \"cluster\"])\n","df[\"sim\"] = [\n","    1 - distance.cosine(model[word], cluster_centroid[cluster])\n","    for word, cluster in word_cluster.items()\n","]\n","df = df.sort_values(by=[\"cluster\", \"sim\"], ascending=[True, False])\n"]},{"cell_type":"code","execution_count":null,"id":"62d759a2","metadata":{"id":"62d759a2"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"id":"talXiL9dRDmk","metadata":{"id":"talXiL9dRDmk"},"outputs":[],"source":["df.to_csv(\"clusters.csv\")"]},{"cell_type":"code","execution_count":null,"id":"90769a22","metadata":{"id":"90769a22"},"outputs":[],"source":["word_cluster[\"attack\"]"]},{"cell_type":"code","execution_count":null,"id":"4b6dcf11","metadata":{"id":"4b6dcf11"},"outputs":[],"source":["word = \"post\"\n","cluster = word_cluster[word]\n","cluster_words[cluster]"]},{"cell_type":"code","execution_count":null,"id":"71507875","metadata":{"id":"71507875"},"outputs":[],"source":["# Create top-3 word labels for each cluster\n","cluster_label = {cluster: \"|\".join(words[:3]) for cluster, words in cluster_words.items()}\n"]},{"cell_type":"code","execution_count":null,"id":"3b349c4d","metadata":{"id":"3b349c4d"},"outputs":[],"source":["cluster_label"]},{"cell_type":"code","execution_count":null,"id":"951505ae","metadata":{"id":"951505ae"},"outputs":[],"source":["def sort_dict(dic, reverse=True):\n","    \"\"\"\n","    Sort a dictionary by its values.\n","\n","    Args:\n","        dic (Dict): Dictionary to sort.\n","        reverse (bool): Whether to sort in descending order.\n","\n","    Returns:\n","        Dict: Sorted dictionary.\n","    \"\"\"\n","    return dict(sorted(dic.items(), key=lambda item: item[1], reverse=reverse))\n","\n","\n","def get_similar_clusters(\n","    word_or_vec, topn=10, least=False\n","):\n","    \"\"\"\n","    Get the most or least similar clusters for a given word or vector.\n","\n","    Args:\n","        word_or_vec (Union[str, np.ndarray]): Word or vector to compare.\n","        topn (int): Number of clusters to return.\n","        least (bool): Whether to return least similar clusters.\n","\n","    Returns:\n","        List[Tuple[int, float]]: List of cluster indices and similarity scores.\n","    \"\"\"\n","    if isinstance(word_or_vec, str):\n","        vector = model[word_or_vec]\n","    elif isinstance(word_or_vec, np.ndarray):\n","        vector = word_or_vec\n","    else:\n","        raise ValueError(\"Input must be a string (word) or a NumPy array (vector).\")\n","\n","    # Compute similarities to all cluster centroids\n","    cluster_similarities = {\n","        cluster: 1 - distance.cosine(vector, centroid)\n","        for cluster, centroid in cluster_centroid.items()\n","    }\n","    sorted_clusters = sort_dict(cluster_similarities, reverse=not least)\n","    return list(sorted_clusters.items())[:topn]\n","\n","def print_similar_clusters(\n","    word_or_vec, topn=10, least=False\n","):\n","    \"\"\"\n","    Print the most or least similar clusters for a given word or vector.\n","\n","    Args:\n","        word_or_vec (Union[str, np.ndarray]): Word or vector to compare.\n","        topn (int): Number of clusters to print.\n","        least (bool): Whether to print least similar clusters.\n","    \"\"\"\n","    clusters = get_similar_clusters(word_or_vec, topn=topn, least=least)\n","    for cluster, similarity in clusters:\n","        if cluster in cluster_label:\n","            print(f\"Cluster {cluster}: {cluster_label[cluster]} (Similarity: {similarity:.3f})\")\n"]},{"cell_type":"code","execution_count":null,"id":"7d927d1b","metadata":{"id":"7d927d1b"},"outputs":[],"source":["print_similar_clusters(\"attack\", topn=20)"]},{"cell_type":"code","execution_count":null,"id":"jrjhhPnGQUwN","metadata":{"id":"jrjhhPnGQUwN"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"dimensions","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}